{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 203/203 [00:00<00:00, 13051.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from fol import evaluate\n",
    "from mlx_lm import load, generate \n",
    "from datasets import load_dataset\n",
    "from folio_dataset import create_prompt, instruction, get_example_prompt_str, fol_to_nltk, convert_to_nltk_rep\n",
    "\n",
    "folio_train = load_dataset('yale-nlp/FOLIO', split='train')\n",
    "folio_val = load_dataset('yale-nlp/FOLIO', split='validation')\n",
    "\n",
    "folio_train = folio_train.map(fol_to_nltk)\n",
    "folio_val = folio_val.map(fol_to_nltk)\n",
    "\n",
    "example_prompt_str = get_example_prompt_str(folio_train, n_shots=4)\n",
    "\n",
    "folio_val = folio_val.map(\n",
    "    create_prompt,\n",
    "    fn_kwargs=dict(\n",
    "        instruction=instruction,\n",
    "        example_prompt_str=example_prompt_str\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [example['label'] for example in folio_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gemma-1.1-7b-it_FOLIO_results.json', 'r') as fp:\n",
    "    results = json.load(fp=fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203, 203)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results), len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3448275862068966"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(labels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[58, 64, 75, 63, 66, 77, 6, 39, 45, 13]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "random.sample([i for i in range(100)], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 0\n",
      "acuracy: 0.4220430107526882\n",
      "precision: 0.2727242369598611\n",
      "recall: 0.4220430107526882\n",
      "f1: 0.24901484480431849\n",
      "depth: 1\n",
      "acuracy: 0.4490861618798956\n",
      "precision: 0.265997637002786\n",
      "recall: 0.4490861618798956\n",
      "f1: 0.22835736290819467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhijiezhu/.pyenv/versions/3.11.8/envs/mlx/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/zhijiezhu/.pyenv/versions/3.11.8/envs/mlx/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 2\n",
      "acuracy: 0.4435336976320583\n",
      "precision: 0.2711435781921328\n",
      "recall: 0.4435336976320583\n",
      "f1: 0.22768681081747058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhijiezhu/.pyenv/versions/3.11.8/envs/mlx/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 3\n",
      "acuracy: 0.4531886024423338\n",
      "precision: 0.2808826780296678\n",
      "recall: 0.4531886024423338\n",
      "f1: 0.22739399074956748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhijiezhu/.pyenv/versions/3.11.8/envs/mlx/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 5\n",
      "acuracy: 0.4453125\n",
      "precision: 0.2839891953959382\n",
      "recall: 0.4453125\n",
      "f1: 0.22041362892426722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhijiezhu/.pyenv/versions/3.11.8/envs/mlx/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import itertools\n",
    "import random\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "import random\n",
    "\n",
    "for depth in range(6):\n",
    "    if depth == 4:\n",
    "        continue\n",
    "    results = ['Unknown'] * len(all_labels)\n",
    "    proofwriter_test = pd.read_json(f'proofwriter-dataset-V2020.12.3/OWA/depth-{depth}/meta-test.jsonl', lines=True)\n",
    "\n",
    "    all_examples = []\n",
    "    for index, row in proofwriter_test.iterrows():\n",
    "        premises = row['theory']\n",
    "        premises = premises.replace('. ', '\\n ')\n",
    "        question_dict = row['questions']\n",
    "        num_q = len(question_dict)\n",
    "        questions, answers = [], []\n",
    "        for i in range(1, num_q + 1):\n",
    "            questions.append(question_dict[f'Q{i}']['question'])\n",
    "            answers.append(question_dict[f'Q{i}']['answer'])\n",
    "        all_examples.append(dict(\n",
    "            premises=premises,\n",
    "            conclusion=questions,\n",
    "            labels=answers\n",
    "        ))\n",
    "    all_examples = all_examples[:100]\n",
    "\n",
    "    labels = [example['labels'] for example in all_examples]\n",
    "\n",
    "    all_labels = list(itertools.chain(*labels))\n",
    "    all_results = ['Unknown'] * len(all_labels)\n",
    "    rand_idxs = random.sample([i for i in range(len(all_labels))], 50)\n",
    "    for idx in rand_idxs:\n",
    "        all_results[idx] = True\n",
    "\n",
    "    print('depth:', depth)\n",
    "    print('acuracy:', accuracy_score(all_labels, all_results))\n",
    "    print('precision:', precision_score(all_labels, all_results, average='weighted'))\n",
    "    print('recall:', recall_score(all_labels, all_results, average='weighted'))\n",
    "    print('f1:', f1_score(all_labels, all_results, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acuracy: 0.3399014778325123\n",
      "precision: 0.2923314462171899\n",
      "recall: 0.3399014778325123\n",
      "f1: 0.17691024357691024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhijiezhu/.pyenv/versions/3.11.8/envs/mlx/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "results = ['Uncertain'] * len(folio_val)\n",
    "results[0] = 'True'\n",
    "results[5] = 'True'\n",
    "labels = [example['label'] for example in folio_val]\n",
    "\n",
    "print('acuracy:', accuracy_score(labels, results))\n",
    "print('precision:', precision_score(labels, results, average='weighted'))\n",
    "print('recall:', recall_score(labels, results, average='weighted'))\n",
    "print('f1:', f1_score(labels, results, average='macro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain',\n",
       " 'Uncertain']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
